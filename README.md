# Voice+Text Chatbot (Practice Repo)

A low-latency voice and text chatbot system built with open-source AI models, featuring real-time speech processing, retrieval-augmented generation (RAG), and a modern web interface.

## ğŸ¯ Project Goals

This is a learning project to implement a production-grade conversational AI system with:
- **Voice Input/Output**: Real-time speech-to-text and text-to-speech
- **Text Chat**: Fast streaming text responses
- **RAG**: Retrieval-Augmented Generation for knowledge-based responses
- **Low Latency**: Optimized for real-time user experience
- **Observability**: Comprehensive monitoring and metrics

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js   â”‚  Landing page with push-to-talk
â”‚   Frontend  â”‚  Real-time streaming UI
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx     â”‚  Reverse proxy & TLS termination
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI    â”‚  Orchestration layer
â”‚     App     â”‚  WebSocket & SSE endpoints
â””â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”˜
   â”‚   â”‚   â”‚
   â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   â”‚            â”‚
â”Œâ”€â”€â–¼â”€â”€â”€â–¼â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”
â”‚     LLM     â”‚  â”‚ RAG  â”‚  Vector search & retrieval
â”‚ (vLLM/llama)â”‚  â”‚      â”‚  FAISS/pgvector
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
   
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ STT  â”‚  â”‚ TTS  â”‚  Speech processing
â”‚Whisperâ”‚  â”‚Piper â”‚  
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Service Level Objectives (SLOs)

Our performance targets for production-quality experience:

| Metric | Target | Description |
|--------|--------|-------------|
| **First Token (Text)** | â‰¤ 300ms | Time to first LLM token for text input |
| **First Token (Voice)** | â‰¤ 700ms | Time to first LLM token for voice input |
| **Audio â†’ Transcript** | â‰¤ 800ms | STT processing latency |
| **Streaming Rate** | â‰¥ 30 tok/s | LLM token generation speed (8B model on GPU) |
| **End-to-End P99** | â‰¤ 2.5s | 99th percentile total response time |
| **Concurrent Users** | â‰¥ 100 | Simultaneous user capacity |
| **Uptime** | 99.9% | System availability |

## ğŸš€ Quick Start

### Prerequisites
- Docker Desktop with GPU support (optional but recommended)
- Node.js 18+
- Python 3.10+
- GitHub CLI

### Setup

1. **Clone and configure:**
   ```bash
   git clone https://github.com/deepspeccode/voicebot-rag-practice.git
   cd voicebot-rag-practice
   cp .env.example .env
   # Edit .env with your configuration
   ```

2. **Start services:**
   ```bash
   docker compose up -d
   ```

3. **Verify health:**
   ```bash
   curl http://localhost:8080/healthz
   ```

4. **Access the UI:**
   - Frontend: http://localhost:3000
   - API: http://localhost:8080

## ğŸ› ï¸ Technology Stack

### AI Services
- **LLM**: vLLM or llama.cpp with Llama 3.1 8B Instruct
- **STT**: faster-whisper (OpenAI Whisper)
- **TTS**: Piper or Coqui-TTS
- **Embeddings**: intfloat/e5-small-v2

### Infrastructure
- **Orchestration**: FastAPI with WebSocket & SSE
- **Vector DB**: FAISS or pgvector
- **Web Server**: Nginx with TLS
- **Frontend**: Next.js 14+ with TypeScript
- **Monitoring**: Prometheus + Grafana

### Deployment
- **Containers**: Docker Compose (dev), Kubernetes (prod)
- **IaC**: Terraform
- **CI/CD**: GitHub Actions

## ğŸ“š Project Phases

This project is organized into 6 phases with 10 tasks:

### Phase 1: Foundations
- âœ… Project setup and infrastructure

### Phase 2: Core AI Services
- ğŸ”² LLM service implementation
- ğŸ”² Speech-to-Text service
- ğŸ”² Text-to-Speech service

### Phase 3: RAG
- ğŸ”² RAG service with vector search

### Phase 4: Orchestration
- ğŸ”² FastAPI coordination layer

### Phase 5: Frontend
- ğŸ”² Next.js UI with push-to-talk

### Phase 6: Observability & Hardening
- ğŸ”² Monitoring and alerting
- ğŸ”² Production deployment
- ğŸ”² Performance optimization

## ğŸ“ˆ Monitoring & Observability

Access monitoring dashboards:
- **Grafana**: http://localhost:3001
- **Prometheus**: http://localhost:9090

Key metrics tracked:
- Response latencies (P50, P95, P99)
- Token generation rates
- Error rates and success rates
- Resource utilization (CPU, Memory, GPU)
- Concurrent connections

## ğŸ§ª Testing

```bash
# Run unit tests
docker compose run --rm app pytest

# Run integration tests
./scripts/integration-test.sh

# Load testing
./scripts/load-test.sh
```

## ğŸ“– Documentation

- [Project Tracking Guide](PROJECT_TRACKING.md)
- [Setup Instructions](project_instructions.md)
- [GitHub Project Board](https://github.com/users/deepspeccode/projects/8)

## ğŸ¤ Contributing

This is a learning project, but contributions are welcome!

1. Create a feature branch: `git checkout -b feat/my-feature`
2. Make your changes with tests
3. Push and create a PR: `gh pr create --fill`

## ğŸ“„ License

MIT License - feel free to use this for learning and experimentation.

## ğŸ™ Acknowledgments

Built with open-source models and tools:
- Meta's Llama models
- OpenAI's Whisper
- vLLM team
- FastAPI and Next.js communities

---

**Status**: ğŸš§ Active Development | **Phase**: 1 - Foundations

