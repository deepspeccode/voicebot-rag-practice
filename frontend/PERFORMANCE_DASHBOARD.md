# Performance Dashboard - Frontend Integration

## 📊 Overview

The frontend now includes a comprehensive performance monitoring dashboard that displays real-time metrics from the LLM service. This provides users with visibility into the system's performance and health status.

## 🎯 Features

### **Main Performance Dashboard**
- **Total Requests**: Shows the cumulative number of successful requests
- **Tokens Generated**: Displays the total number of tokens generated by the model
- **Average Response Time**: Shows the average response time in milliseconds

### **Detailed Performance Panel**
- **Success Rate**: Percentage of successful requests (typically 100%)
- **Requests/sec**: Current throughput rate
- **Last Request**: Timestamp of the most recent request
- **Model Status**: Shows if the model is Active, Loading, or Offline
- **Uptime**: How long the service has been running
- **Memory Usage**: Current memory consumption (~8GB for Qwen2.5 7B)

## 🔧 Technical Implementation

### **Data Sources**
1. **Prometheus Metrics** (`/metrics` endpoint):
   - `llm_requests_total`: Total request count
   - `llm_tokens_total`: Total tokens generated
   - `llm_request_duration_seconds_sum`: Response time data

2. **Health Status** (`/healthz` endpoint):
   - Service status and uptime
   - Model loading status

### **Real-time Updates**
- **Automatic Refresh**: Metrics update every 30 seconds
- **Manual Refresh**: "🔄 Refresh Metrics" button for immediate updates
- **Live Tracking**: Response times and token counts tracked per request

### **User Interface**
- **Responsive Design**: Works on desktop and mobile devices
- **Color-coded Status**: Green (healthy), Yellow (loading), Red (offline)
- **Expandable Details**: Toggle between basic and detailed metrics
- **Visual Indicators**: Icons and progress indicators for better UX

## 📈 Performance Metrics Explained

### **Request Metrics**
- **Total Requests**: Cumulative count of all successful API calls
- **Success Rate**: Percentage of requests that completed successfully
- **Requests/sec**: Current throughput (typically ~1.6 req/s for CPU-only setup)
- **Last Request**: Timestamp of the most recent interaction

### **Model Performance**
- **Model Status**: 
  - 🟢 **Active**: Model loaded and ready to respond
  - 🟡 **Loading**: Model is being loaded or initialized
  - 🔴 **Offline**: Service is unavailable
- **Uptime**: Service runtime in hours, minutes, and seconds
- **Memory Usage**: Current RAM consumption (optimized to ~8GB)

### **Response Metrics**
- **Average Response Time**: Mean response time across all requests
- **Tokens Generated**: Total tokens produced by the model
- **Response Quality**: Real-time streaming performance

## 🚀 Usage

### **Basic Monitoring**
1. Open the frontend in your browser
2. The performance dashboard loads automatically
3. Metrics update in real-time as you chat

### **Detailed Analysis**
1. Click "📈 Show Detailed Performance" to expand the panel
2. View comprehensive metrics and status information
3. Click "📉 Hide Detailed Performance" to collapse

### **Manual Refresh**
1. Click "🔄 Refresh Metrics" to update immediately
2. Useful for troubleshooting or immediate status checks

## 🔍 Troubleshooting

### **Metrics Not Updating**
- Check if the LLM service is running (`http://localhost:8001`)
- Verify the `/metrics` endpoint is accessible
- Check browser console for JavaScript errors

### **Status Shows Offline**
- Ensure Ollama is running (`ollama serve`)
- Verify the Qwen2.5 7B model is loaded
- Check network connectivity to localhost:8001

### **Performance Issues**
- Monitor response times - should be ~770ms average
- Check memory usage - should be around 8GB
- Review token generation rate - should be ~33 tok/s

## 📊 Expected Performance Values

### **CPU-Only Setup (Current)**
- **Response Time**: ~770ms average
- **Token Rate**: ~33 tokens/second
- **Memory Usage**: ~8GB RAM
- **Throughput**: ~1.6 requests/second
- **Success Rate**: 100%

### **Future GPU Setup (Target)**
- **Response Time**: <100ms average
- **Token Rate**: >100 tokens/second
- **Memory Usage**: <16GB VRAM
- **Throughput**: >10 requests/second
- **Success Rate**: 100%

## 🎨 Customization

The performance dashboard can be customized by modifying:
- **Update Intervals**: Change the 30-second refresh rate
- **Metrics Display**: Add or remove specific metrics
- **Visual Styling**: Modify colors, icons, and layout
- **Alert Thresholds**: Add performance warnings or alerts

## 🔗 Integration

The performance dashboard integrates seamlessly with:
- **Prometheus Monitoring**: Direct metrics consumption
- **Health Checks**: Real-time status monitoring
- **Chat Interface**: Live performance tracking during conversations
- **Future Monitoring**: Ready for Grafana or other monitoring tools

This performance dashboard provides comprehensive visibility into the LLM service performance, helping users understand system behavior and identify optimization opportunities.
