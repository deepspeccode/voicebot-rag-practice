{
  "model_name": "llama-3.1-8b-instruct",
  "model_file": "llama-3.1-8b-instruct.Q4_K_M.gguf",
  "model_path": "/models/llama-3.1-8b-instruct.Q4_K_M.gguf",
  "quantization": "Q4_K_M",
  "context_size": 2048,
  "max_tokens": 512,
  "temperature": 0.7,
  "top_p": 0.9,
  "threads": 4,
  "batch_size": 512,
  "n_gpu_layers": 0,
  "description": "Llama 3.1 8B Instruct model with Q4_K_M quantization for CPU inference"
}
